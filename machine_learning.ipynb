{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Detection & Classification using machine learning",
   "id": "c49afadeeb0f2eff"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Building the dataset ",
   "id": "c8ba37ae5162b152"
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "seed = 42\n",
    "image_folder = 'dataset/train/images/'\n",
    "label_folder = 'dataset/train/labels/'\n",
    "target_image_folder = 'dataset/train/images_aug/'\n",
    "target_label_folder = 'dataset/train/labels_aug/'"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### DATA AUGMENTATION",
   "id": "55d594bfbca77145"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from src.augment import ImageBatchProcessor\n",
    "processor = ImageBatchProcessor(image_folder=image_folder, label_folder=label_folder, target_image_folder=target_image_folder, target_label_folder=target_label_folder)\n",
    "processor.augment_and_save(nb_augmentation=5)"
   ],
   "id": "719aac8c3075b0db",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Image cropping and negative examples generation",
   "id": "91475a47a8808751"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from src.crop import crop_images_from_folder\n",
    "crop_images_from_folder(target_image_folder, target_label_folder, 'dataset/train/aug_cropped_images/')\n",
    "crop_images_from_folder(image_folder, label_folder, 'dataset/train/cropped_images/')\n",
    "val_path = 'dataset/val/images/'\n",
    "val_label_path = 'dataset/val/labels/'\n",
    "crop_images_from_folder(val_path, val_label_path, 'dataset/val/cropped_images/')"
   ],
   "id": "23549e8f58d80f89",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Dataset building and preprocessing",
   "id": "939b737d745a625e"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from src.dataset import dataset\n",
    "val_path = 'dataset/val/cropped_images/'\n",
    "aug_train_path = 'dataset/train/aug_cropped_images/'\n",
    "train_path = 'dataset/train/cropped_images/'\n",
    "standard_size = (64, 64)\n",
    "label_size_factor = 1\n",
    "train_data = dataset(img_dir=train_path, augment_path=aug_train_path, label_size_factor=label_size_factor, standard_size=standard_size)\n",
    "val_data = dataset(val_path, train=False, standard_size=standard_size)\n",
    "label_count = {}\n",
    "for x in train_data.images:\n",
    "    label = x.label\n",
    "    if label in label_count.keys():\n",
    "        label_count[label] +=1\n",
    "    else:\n",
    "        label_count[label] =1\n",
    "        \n",
    "for x in val_data.images:\n",
    "    label = x.label\n",
    "    if label in label_count.keys():\n",
    "        label_count[label] +=1\n",
    "    else:\n",
    "        label_count[label] =1\n",
    "label_count"
   ],
   "id": "b2570936b27cdc2b",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## First Model Training",
   "id": "f17da5daae868958"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from src.model import model\n",
    "max_iter = 10000\n",
    "# initialize the model\n",
    "my_model = model(seed)\n",
    "#training\n",
    "print(f\"Training model {my_model.name} with seed {seed},max_iter {max_iter}, standard_size {standard_size}, label_size_factor {label_size_factor}\")\n",
    "my_model.train_svm(train_data,val_data, verbose=0, max_iter=max_iter)\n",
    "accuracy = my_model.evaluate(val_data)"
   ],
   "id": "f6023224c23f1083",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## First Detection on test",
   "id": "5042ff7764a33da2"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from detection import detection_images_in_folder\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\",\n",
    "                        message=\"Applying `local_binary_pattern` to floating-point images may give unexpected results when small numerical differences between adjacent pixels are present.\")\n",
    "\n",
    "image_path = \"dataset/test/images\"\n",
    "detection_images_in_folder(image_path, my_model, 'detections.csv')"
   ],
   "id": "12b12d2d3c63a3a",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Augmentating train none label with the false positive of other labels",
   "id": "22bc259b68f72328"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "from detection import build_detection_validation\n",
    "label_path = \"dataset/train/labels\"\n",
    "build_detection_validation(label_path)\n",
    "# Load the data\n",
    "validations = pd.read_csv('validations.csv', header=None)\n",
    "detections = pd.read_csv('detections.csv', header=None)\n",
    "# Filter out the 'ff' labels\n",
    "validations = validations[validations.iloc[:, 5] != 'ff']\n",
    "detections = detections[detections.iloc[:, 6] != 'ff']\n",
    "# Function to check if a value can be converted to float\n",
    "def is_float(value):\n",
    "    try:\n",
    "        float(value)\n",
    "        return True\n",
    "    except ValueError:\n",
    "        print(f\"{value} rejected\")\n",
    "        return False\n",
    "validations = validations[validations.iloc[:, 1:5].map(is_float).all(axis=1)]\n",
    "detections = detections[detections.iloc[:, 1:5].map(is_float).all(axis=1)]\n",
    "validations.iloc[:, 1:5] = validations.iloc[:, 1:5].astype(float)\n",
    "detections.iloc[:, 1:5] = detections.iloc[:, 1:5].astype(float)\n",
    "detections.iloc[:, 5] = detections.iloc[:, 5].astype(float)\n",
    "\n",
    "from gen_neg import get_negative_prediction\n",
    "negative_samples_dir = \"dataset/train/cropped_images\"\n",
    "image_folder_path = \"dataset/train/images\"\n",
    "get_negative_prediction(detections, validations, image_folder_path, negative_samples_dir)"
   ],
   "id": "6943d1f3c5344a3d",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Second model training",
   "id": "313be8891f5131fd"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "#reload and process the datasets\n",
    "train_data = dataset(img_dir=train_path, augment_path=aug_train_path, label_size_factor=label_size_factor, standard_size=standard_size)\n",
    "val_data = dataset(val_path, train=False, standard_size=standard_size)\n",
    "my_model = model(seed)\n",
    "#training\n",
    "print(f\"Training second model {my_model.name} with seed {seed},max_iter {max_iter}, standard_size {standard_size}, label_size_factor {label_size_factor}\")\n",
    "my_model.train_svm(train_data,val_data, verbose=0, max_iter=max_iter)\n",
    "accuracy = my_model.evaluate(val_data)"
   ],
   "id": "40771bcd73494904",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Second Detection & Augmentation on test\n",
   "id": "108ed95ed0c1ff41"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "detection_images_in_folder(image_path, my_model, 'detections.csv')\n",
    "build_detection_validation(label_path)\n",
    "detections = pd.read_csv('detections.csv', header=None)\n",
    "# Filter out the 'ff' labels\n",
    "detections = detections[detections.iloc[:, 6] != 'ff']\n",
    "detections = detections[detections.iloc[:, 1:5].map(is_float).all(axis=1)]\n",
    "detections.iloc[:, 1:5] = detections.iloc[:, 1:5].astype(float)\n",
    "detections.iloc[:, 5] = detections.iloc[:, 5].astype(float)\n",
    "get_negative_prediction(detections, validations, image_folder_path, negative_samples_dir)"
   ],
   "id": "4154e352b986c223",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Third Model Training",
   "id": "425b391b5705ee19"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "#reload and process the datasets\n",
    "train_data = dataset(img_dir=train_path, augment_path=aug_train_path, label_size_factor=label_size_factor, standard_size=standard_size)\n",
    "val_data = dataset(val_path, train=False, standard_size=standard_size)\n",
    "my_model = model(seed)\n",
    "#training\n",
    "print(f\"Training third model {my_model.name} with seed {seed},max_iter {max_iter}, standard_size {standard_size}, label_size_factor {label_size_factor}\")\n",
    "my_model.train_svm(train_data,val_data, verbose=0, max_iter=max_iter)\n",
    "accuracy = my_model.evaluate(val_data)"
   ],
   "id": "2894d4b5f5b487ae",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Final detection on validation dataset",
   "id": "e1e1937a1925589f"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "image_path = \"dataset/val/images\"\n",
    "label_path = \"dataset/val/labels\"\n",
    "\n",
    "detection_images_in_folder(image_path, my_model, 'detections.csv')\n",
    "build_detection_validation(label_path)\n",
    "validations = pd.read_csv('validations.csv', header=None)\n",
    "detections = pd.read_csv('detections.csv', header=None)\n",
    "validations = validations[validations.iloc[:, 5] != 'ff']\n",
    "detections = detections[detections.iloc[:, 6] != 'ff']\n",
    "validations = validations[validations.iloc[:, 1:5].map(is_float).all(axis=1)]\n",
    "detections = detections[detections.iloc[:, 1:5].map(is_float).all(axis=1)]\n",
    "validations.iloc[:, 1:5] = validations.iloc[:, 1:5].astype(float)\n",
    "detections.iloc[:, 1:5] = detections.iloc[:, 1:5].astype(float)\n",
    "detections.iloc[:, 5] = detections.iloc[:, 5].astype(float)"
   ],
   "id": "40d7e74e8d0d089a",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Results",
   "id": "2c5149796d925c31"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import confusion_matrix\n",
    "# Initialize lists to store y (ground truth) and y_pred (predictions)\n",
    "y = []\n",
    "y_pred = []\n",
    "matching_validation_row = []\n",
    "fp_data = []\n",
    "\n",
    "# Define IoU function\n",
    "def iou(box1, box2):\n",
    "    # Unpack the coordinates\n",
    "    x1_min, y1_min, x1_max, y1_max = box1\n",
    "    x2_min, y2_min, x2_max, y2_max = box2\n",
    "\n",
    "    # Calculate the intersection coordinates\n",
    "    inter_x_min = max(x1_min, x2_min)\n",
    "    inter_y_min = max(y1_min, y2_min)\n",
    "    inter_x_max = min(x1_max, x2_max)\n",
    "    inter_y_max = min(y1_max, y2_max)\n",
    "\n",
    "    # Calculate the intersection area\n",
    "    inter_area = max(0, inter_x_max - inter_x_min) * max(0, inter_y_max - inter_y_min)\n",
    "\n",
    "    # Calculate the areas of each box\n",
    "    box1_area = (x1_max - x1_min) * (y1_max - y1_min)\n",
    "    box2_area = (x2_max - x2_min) * (y2_max - y2_min)\n",
    "\n",
    "    # Calculate the union area\n",
    "    union_area = box1_area + box2_area - inter_area\n",
    "\n",
    "    # Calculate the IoU\n",
    "    iou = inter_area / union_area\n",
    "\n",
    "    return iou\n",
    "\n",
    "# Match detections with ground truth\n",
    "for _, detection_row in detections.iterrows():\n",
    "    image_id_d, x_min_d, y_min_d, x_max_d, y_max_d, score_d, label_d = detection_row\n",
    "    max_iou = 0.5\n",
    "    matching_validation = 'none'\n",
    "\n",
    "    for idx, validation_row in validations.iterrows():\n",
    "        image_id_v, x_min_v, y_min_v, x_max_v, y_max_v, label_v = validation_row\n",
    "        \n",
    "        if image_id_v == image_id_d and label_v == label_d:\n",
    "            iou_value = iou((x_min_v, y_min_v, x_max_v, y_max_v), (x_min_d, y_min_d, x_max_d, y_max_d))\n",
    "            \n",
    "            if iou_value > max_iou:\n",
    "                max_iou = iou_value\n",
    "                matching_validation = label_v\n",
    "                \n",
    "                matching_validation_row.append(idx)\n",
    "    \n",
    "    # Add ground truth and prediction based on maximum IoU\n",
    "    y_pred.append(label_d)\n",
    "    y.append(matching_validation)\n",
    "    \n",
    "    if matching_validation == 'none':\n",
    "        fp_data.append(list(detection_row))\n",
    "\n",
    "for idx, validation_row in validations.iterrows():\n",
    "    image_id_v, x_min_v, y_min_v, x_max_v, y_max_v, label_v = validation_row\n",
    "    if idx not in matching_validation_row:\n",
    "        y.append(label_v)\n",
    "        y_pred.append('none')\n",
    "\n",
    "# Plot confusion matrix\n",
    "cm = confusion_matrix(y, y_pred, normalize='true')\n",
    "# Get unique labels\n",
    "unique_labels = sorted(list(set(y)))\n",
    "\n",
    "plt.figure(figsize=(10, 7))\n",
    "sns.heatmap(\n",
    "    cm,\n",
    "    annot=True,\n",
    "    fmt=\".2%\",\n",
    "    xticklabels=unique_labels,\n",
    "    yticklabels=unique_labels,\n",
    "    cmap=\"Blues\",\n",
    ")\n",
    "plt.xlabel(\"Predicted\")\n",
    "plt.ylabel(\"Truth\")\n",
    "plt.show()\n",
    "\n",
    "# Calculate precision and recall\n",
    "precision = {}\n",
    "recall = {}\n",
    "\n",
    "for i in range(len(unique_labels)):\n",
    "    cls = unique_labels[i]\n",
    "    tp = cm[i, i]\n",
    "    fp = np.sum(cm[:, i]) - tp\n",
    "    fn = np.sum(cm[i, :]) - tp\n",
    "    \n",
    "    precision[cls] = tp / (tp + fp) if tp + fp > 0 else 0\n",
    "    recall[cls] = tp / (tp + fn) if tp + fn > 0 else 0\n",
    "\n",
    "print(\"Precision:\\n\", precision)\n",
    "print(\"\\nRecall:\\n\", recall)\n",
    "\n",
    "tp_total = np.sum(np.diag(cm))\n",
    "fp_total = np.sum(cm) - tp_total\n",
    "fn_total = np.sum(cm) - tp_total\n",
    "\n",
    "overall_precision = tp_total / (tp_total + fp_total) if tp_total + fp_total > 0 else 0\n",
    "overall_recall = tp_total / (tp_total + fn_total) if tp_total + fn_total > 0 else 0\n",
    "\n",
    "print(\"\\nOverall precision:\\n\", overall_precision)\n",
    "print(\"\\nOverall recall:\\n\", overall_recall)"
   ],
   "id": "853a212609d2ae9e",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
